---
title: "Project"
author: "Bruno Contrino"
date: "2 June 2016"
output: html_document
---


In the paper *'Combining Statistical Evidence From Several Studies: A Method Using Bayesian Updating and an Example From Research on Trust Problems in Social and Economical Exchange'* by Kuiper et al, the authors use Bayes factors to combine a series of Studies and quantify the joint evidence in multiple studies regarding the effects of one variable.

The method will use Bayes Factors which quanitfy evidence for several hypotheses. Th Bayes factor gives the relative support for each hypothesis allowing one to make statements such as the following  $H_{>}$ is $x$ times more likely that $H_{<}$. 

The input for Bayesian updating is the estimate of the parameter of interest and its standard error which can be otained from datasets or directly form published studies. Something that must be considered is that it is the person who implements the method which must decide whether the different studies or data sets which will be studied do indeed offer a comparable relationship between two key variables. 


### The method 

In this section I will give a detailed walkthrough of the method which will then be followed by a walkthrough of the code, which will be less detailed but may contain the most parts which I believe help clarify the code in as well. 

Evidence will be calculated for three hypotheses: $H_{0}$ , $H_{>}$,  $H_{<}$ where: 

$$
\begin{aligned}
H_{0} &= \textrm{No effect} \\
H_{>} &= \textrm{Positive effect} \\
H_{<} &= \textrm{Negative effect} \\
\end{aligned}
$$


#### Step One: Define Prior Model Probabilities 

The prior model probability (PrMP), denoted as $\pi^{0}_{m} \,  \textrm{for} \, m\, \in  \{0,>,< \}$, is a number on a scale of 0 to 1, which quantifies the weight attached to the current hypothesis. Subsequently, we start with one study and use its parameter estimate and standard error to calculate or approximate the likelihood. Based on this, the Bayes factor for each hypothesis is determined.

To initialise, the PrMPs are given equal weight and thus: $\pi_{m}^{0} = \frac{1}{3}$. 

#### Step Two: Computing Prior Variance

To evaluate the hypotheses $H_{m} \,  \textrm{for} \, m\, \in  \{0,>,< \}$ the Distribution of $H_{m}$ needs to be specified. Kuiper et al use a conjugate prior (Gelman et al. 2004), which implies that the prior distribution of the parameter $\beta_{1} is normal, which in turn will result in a normal posterior. To determine the prior distribution of $H_{m}$ one needs to consider the case in which the parameter $\beta_{1}$ for the unconstrained case. 

The unconstrained prior is defined by: $P(\beta_{1}) = \mathcal{N}(\beta_{0}, \sigma_{0}^{2})$

Becasue the belief in either the positive or negative effect should be the same we set $\beta_{0} = 0$. Therefore we now need to calculate the prior variance in order to have the parameters for the unconstrained prior. 

The literature (Kuiper et al and others) suggest that the prior variance be 'vague/non-informative such that is, such that it has little influence on the results. But, it should not be too vague, because then $H_{0}$ will receive the highest support also when it is not true. This is known as the Lindley paradox (Lindley 1957). Grounding the prior variance on the data avoids having too vague priors (Berger and Pericchi 2004, 1996).' 

The method used by Kuiper et al is propesed by Klugkist, Laudy, and Hoijtink (2005) and is as follows:

- Calculate the 99% confidence intervals of the observed mean and standard deviation. 

- Choose the bound which will maximise the difference from the prior mean (zero in our case).

- Use this to set the prior variance. 

#### Step Three: Use Prior variance and mean to calculate the unconstrained posterior distribution parametrs

Once the prior variances have been calculated, they can be used along side the prior mean $\beta_{0}$ and the observed means and variances to calculate the unconstrained posterior means and variances. These are computed for $t = 1, ... ,T$ where $T = \textrm{Total number of Studies}$ , using the following: 

$$
\beta^{'}_{t} = \frac{\frac{\hat{\beta}_{t}}{\hat{\sigma}_{t}^{2}}+\frac{\beta_{0}}{\sigma^{2}_{0,t}}}{\frac{1}{\hat{\sigma}^{2}_{t}}+\frac{1}{\sigma^{2}_{0,t}}} \, , \, \sigma^{'2}_{t} = \frac{1}{\frac{1}{\hat{\sigma}^{2}_{t}} + \frac{1}{\sigma^{2}_{0,t}}}
$$


Where $\hat{\beta}_{t}$ is the observed mean and  $\hat{\sigma}^{2}_{t}$ is the observed variance for study $t$. Then the unconstrained posterior distribution will be $\mathcal{N}(\beta^{'}, \sigma^{'2})$. 


#### Step Four: Calculating Bayes Factors for a constrained Hypothesis with respect to the unconstrained hypothesis. 

Once the unconstrained prior and posterior distribution parameters have been calculated, the Bayes Factors for a constrained Hypothesis with respect to the unconstrained hypothesis can be calculated. These will be denoted as $BF_{m,u} \,  \textrm{for} \, m\, \in  \{0,>,< \}$.

Kuiper et al describe these values as a 'useful technical tool' as the unconstrained hypothesis isn't considered. 

The reason it is useful is because the Bayes Factors with respect to other hypotheses can be calculated from these values via the following calculation: $H_{m,} \, , H_{m',} \,  \textrm{for} \, m \, , m^{'} \, \in  \{0,>,< \}$: 

$$
BF_{m,m^{'}} = \frac{BF_{m,u}}{BF_{m^{'},u}}
$$

To calculate $BF_{m,u}  \textrm{ for } m \in \{ >,<\}$:

$$
BF_{m,u} = \frac{f_{m}}{c_{m}}
$$

Where $c_{m} \textrm{ and } f_{m}$ are the proportions of the unconstrained prior in  $P(\beta_{1}) = \mathcal{N}(\beta_{0}, \sigma_{0}^{2})$ and unconstrained posterior in $P( \beta_{1} | y) \approx \mathcal{N}(\beta^{'},\sigma^{'})$ respectively, in agreement with constraints of hypothesis $H_{m}\textrm{ for } m \in \{ >,<\}$.

Now we chose $\beta_{0} = 0$ such that half of the prior distribution would be in agreement with $H_{>}$ and the other half would be in agreement with $H_{<}$. Therefore by definition $c_{m} = \frac{1}{2}$.

To calculate $BF_{0,u}$ I use a result by Dickey (1971):

$$
BF_{0,u} = \frac{P(\beta_{1} = 0 | y)}{P(\beta_{1} = 0)}
$$

So one can calculate $BF_{0,u}$ by evaluationg the unconstrained posterior and prior density at $\beta_{1} = 0$.


#### Step Five: Calculating Posterior Model Probabilities

The Posterior Model Probability for a given hypothesis $H_{m}$ gives the relative support for $H_{m}$ for a finite set of hypotheses $m$. The PMP $\pi^{1}_{m}$ is calculated as follows:

$$
\pi^{1}_{m} = \frac{\pi^{0}_{m} BF_{mu}}{\pi^{0}_{0} BF_{0u} + \pi^{0}_{>} BF_{>u} + \pi^{0}_{<} BF_{<u}}
$$

Where $\pi^{0}_{m}$ is the Prior Model Probbility which, for the first step, is initialised as $\frac{1}{3}$ (see step 1).

#### Step Six: Updating Posterior Model Probabilities

Now the evidence, for each hypothesis, from multiple studies must be combined. This is done by setting the PrMP of the study $t$ to the PMP of the study $t-1$. Then the updating step follows from this: 

$$
\pi^{0}_{t,m} = \pi^{1}_{t-1,m} \, \textrm{for }  t = 2, ... ,T
$$

Where $\pi^{1}_{t,m}$ is defined in the step above. 

This concludes the method.

Please see below for a full walkthrough of all the functions which have been written to carry out the above.

**Code for calculation posterior model probabilities using Bayes factors**

The various functions presented will be in order of when they are needed. For clarity on outputs and general flow I will use the real data example of Kuiper et al. Some of the outputs will be directly comparable to their published results. Please note the ares which the functoins are referenced to are where they are described in the paper and not where their results are explained.

Below are the input parameter values I will use: 

```{r examplesetup, echo = FALSE}

beta0<- 0
beta1<- c(0.090,0.140,1.090,1.781)
var1<- c(0.029,0.054,0.093,0.179)
var1<- var1^2
pi0<- rep(1/3,3)

```

##### Computing Prior Variance: *prior.var(beta,sigma, percent = 99)*

This function computes the prior variance, The literature (Kuiper et al and others) suggest that the prior variance be 'vague/non-informative such that is, such that it has little influence on the results. But, it should not be too vague, because then $H_{0}$ will receive the highest support also when it is not true. This is known as the Lindley paradox (Lindley 1957). Grounding the prior variance on the data avoids having too vague priors (Berger and Pericchi 2004, 1996).' 

The method used by Kuiper et al is propesed by Klugkist, Laudy, and Hoijtink (2005) and is as follows:

- Calculate the 99% confidence intervals of the observed mean and standard deviation. 

- Choose the bound which will maximise the difference from the prior mean ( zero in out case).

- Use this to set the prior variance. 


```{r priorvar}

## variance of prior 
## REFERENCE : Combining Statistical evidence from several Studies , Kuiper et al, page 9 (69-70) and Figure 2.
#beta  = vector of parameter estimates where beta[i] is for ith study 
#sigma  = vector of standard errors where sigma[i] is for ith study 

prior.var<- function(beta,sigma, percent = 99){
  #setup
  data<- matrix(0, ncol = 2 , nrow = length(beta))
  percent<- (percent / 100)
  percent<- percent + (1-percent)/2
  multiplier<- qnorm(percent)
  # get min/max
  data[,1]<- beta - sigma*multiplier
  data[,2]<- beta + sigma*multiplier
  
  bounds<- abs(data)
  output <- apply(X = bounds,MARGIN = 1,max)
  output <- ((output / multiplier))^2 
  output
}


```

**Summary of input / output**

__Inputs__

- beta: The observed mean of the data / studies.

- sigma: The observed standard deviation of the data / studies.

- percent = 99: This sets the confidence intervals used to calculate the prior variances. The default is a 99% C.I.

__Outputs__

The output consists of a vector which has length as the number of studies. The prior vriances are ordered in this vector such that position $i$ represents the prior variance of study $i$. 

**Example Output**

```{r expriorvar, echo = FALSE}

var0<- prior.var(beta1,sigma = sqrt(var1))
var0

```

After running this function we have the parameters of the unconstrained prior $P(\beta_{1})   \textrm{~} \mathcal{N}(\beta_{0} , \sigma_{0}^2)$

$$
\\
$$


####Calculating Unconstrained Posterior Parameters: *u.post.param(prior.beta,prior.var,data.beta,data.var)*

The following function is used to calculate the posterior means and variations for the observed data. The following equations are used in order to do this: NEED TO ADD STUDY INDEX (DIFFERENT TO PAPER BUT MORE CLEAR)

$$
\beta^{'} = \frac{\frac{\hat{\beta}}{\hat{\sigma}^{2}}+\frac{\beta_{0}}{\sigma^{2}_{0}}}{\frac{1}{\hat{\sigma}^{2}}+\frac{1}{\sigma^{2}_{0}}} , \sigma^{'} = \frac{1}{\frac{1}{\hat{\sigma}^{2}} + \frac{1}{\sigma^{2}_{0}}}
$$


```{r upostparam}

## Unconstrained posterior parameters 
## REFERENCE : Combining Statistical evidence from several Studies , Kuiper et al, page 9 (70) .


u.post.param<- function(prior.beta,prior.var,data.beta,data.var){

  beta.var<- 1/ ((1/prior.var)  + (1/data.var)) 
  beta.tilda<-(prior.beta * (1/prior.var)  + data.beta * (1/data.var))*beta.var
  
  out<- rbind(beta.tilda,beta.var)
  rownames(out)<- c("u.beta.post","u.var.post")
  out
}

```


**Summary of input / output**

__Inputs__

prior.beta: This is the prior mean $\beta_{0}$ which I set to 0. 

prior.var: Takes the prior variances.This is the output produced from the prior.var function.

data.beta: This is the observed means of the studies. 

data.var: This is the observed variances of the studies. 

__Outputs__

The output of this function is a $2 \times T$ matrix where T is the number of studies. 
The first row consists of the unconstrained posterior model mean for each study $t \in T$.
The second row consists of the unconstrained posterior model variance for each study $t \in T$.

**Example Output**


```{r exupostvar, echo = FALSE}

u.post.variables <- u.post.param(prior.beta = beta0,prior.var = var0,data.beta = beta1,data.var = var1) 
u.post.variables
u.post.variables <- u.post.param(beta0,var0,beta1,var1) 
beta1.post<- u.post.variables[1,]
var1.post<- u.post.variables[2,]
```

$$
\\
$$


####Getting Proportions of Unconstrained Prior: *sampl.data2(n,prior.mean,posterior.mean,prior.sd,posterior.sd,hypothesis = 1)*

The following function gets the proportions of the unconstrained posterior ($f_{m}$) and the proportions of the unconstrained prior ($c_{m}$). These are required to calculate $BF_{mu}, m \in \{ >,<\}$ which is the bayes factor for a constrained hypothesis e.g $H_{>}$ with respect to the unconstrained hypothesis.

```{r sampldata}
## Unconstrained posterior proportions and Unconstrained prior proportions 
## REFERENCE : Combining Statistical evidence from several Studies , Kuiper et al, page 72.
sampl.data2<- function(n,prior.mean,posterior.mean,prior.sd,posterior.sd,hypothesis = 1){
  if(hypothesis == "1"){
    "%a%" = function(x,y){x > y}
  } else { 
    "%a%" = function(x,y) {
      x < y
    }
  }
  
  n.post<- length(posterior.mean)
  Cm<- Fm<- names.vec<- rep(0,n.post)

  Cm[]<- 0.5
  
  for(i in 1:n.post){
    Fm[i]<- mean(ifelse(rnorm(n,posterior.mean[i],posterior.sd[i]) %a% prior.mean , 1 , 0))
    
    names.vec[i]<- paste("prob.post",i,sep = "")
    
  }
  
  out<- matrix(0,ncol = (n.post), nrow = 2)
  rownames(out)<- c("Fm","Cm")
  colnames(out)<- names.vec
  out[2,]<- Cm
  out[1,]<- Fm
  out
}

```

**Summary of input / output**


__Inputs__


- n: Number of samples taken when running rnorm.

- prior.mean : Vctor of prior means.

- posterior.mean : vector of posterior means.

- prior.sd: Vector of prior standard deviations.

- posterior.sd: Vector of posterior standard deviations. 

- hypothesis = 1: This acts as a switch for the hypothesis sampled, can be changed to any other number and hypothesis 2 will be sampled instead. It isn't a crucial input and is more for flexibility and completeness. The analysis will work if this is never changed.

__Outputs__

The output of this function is a $2 \times T$ matrix where T is the number of studies. 
The first row consists of the unconstrained posterior model proportions for each study $t \in T$.
The second row consists of the unconstrained prior model proportions for each study $t \in T$.

**Example Output**

```{r exsmpldata, echo = FALSE}
temp.proportions<- sampl.data2(n = 100000 ,prior.mean =  0, posterior.mean = beta1.post, prior.sd = sqrt(var0) , posterior.sd = sqrt(var1.post),hypothesis = 1)
temp.proportions
```

$$
\\
$$


####Calculating the unconstrained Bayes factors $H_{mu}$: *BF.mu.func(prior.mean,posterior.mean,sd.prior,sd.posterior,n = 1000)*

This function utilises the $c_{m}$ and $f_{m}$ produced in the previous function in order to calculate $BF_{mu} = \frac{f_{m}}{c_{m}}$ for $m \in \{ >, < \}$.

To $BF_{0u}$, the following formula is used in the function: $BF_{0u} = \frac{P(\beta_{1} = 0 |y )}{P(\beta_{1} = 0)}$ so one only has to compute the unconstrained prior and posterior at $beta_{1} = 0$. 


```{r bfmu}
## Calculating unconstrained Bayes Factors. 
## REFERENCE : Combining Statistical evidence from several Studies , Kuiper et al, page 72.

BF.mu.func<- function(prior.mean,posterior.mean,sd.prior,sd.posterior,n = 1000){
  #BF_0u NULL
  n.studies<- length(posterior.mean)
  name.vec<- rep(NA,n.studies)
  for(i in 1:n.studies){
    name.vec[i]<- paste0("study",i)
  }
  BF.0u<- rep(0,n.studies)
  for( i in 1:n.studies){
    BF.0u[i]<- dnorm(prior.mean,posterior.mean[i],sd.posterior[i])/dnorm(prior.mean,prior.mean,sd.prior[i]) 
  }
  
  ## Run sampler for BF.1u
  temp.proportions<- sampl.data2(n,prior.mean,posterior.mean,sd.prior,sd.posterior,hypothesis = 1)
  # BF_1u in the case m = > 
  
  ### CHECK 
  BF.1u<- temp.proportions[1,] /  temp.proportions[2,]
  
  # BF.2u in the case m = <
  
  BF.2u<- (1 - temp.proportions[1,]) / temp.proportions[2,]
  ### CHECK 
 
  out<- rbind(BF.0u,BF.1u,BF.2u)
  rownames(out)<- c("H:0","H:>","H:<")
  colnames(out)<- name.vec
  out
}

```

**Summary of input / output**

__Inputs__

- prior.mean: vector of prior means.

- posterior.mean: vector of posterior means.

- sd.prior: vector of prior standard deviations.

- sd.posterior: vector of posterior standard deviations.

- n = 1000 : Number of random samples taken (used in sampl.data2).  

__Outputs__

The output of this function is a $3 \times T$ matrix where T is the number of studies. 

The first row consists of the Bayes Factor $BF_{0u}$ for the hypothesis $H_{0}$ and the unconstrained model for each study $t \in T$.

The second row row consists of the Bayes Factor $BF_{>u}$ for the hypothesis $H_{>}$ and the unconstrained model for each study $t \in T$.

The third row consists of the Bayes Factor $BF_{<u}$ for the hypothesis $H_{<}$ and the unconstrained model for each study $t \in T$.

**Example Output**

```{r exbfmu, echo = FALSE}
BF.mu<- BF.mu.func(prior.mean =  0, posterior.mean = beta1.post,sd.prior = sqrt(var0) , sd.posterior = sqrt(var1.post),n = 100000 )
BF.mu
```


####Calculating Posterior Model Probabilities (PMP) from Prior Model Probabilities

This section will consist of two functions: **pi0.to.1** and **pi.0m.t.func** 

**pi0.to.1**

This function calculates a prior model probability ($\pi^{1}_{m}$)for a hypothesis $H_{m}$. Where $\pi^{1}_{m}$ gives the relative support for $H_{m}$ in a finite set of hypotheses.

The function does the following: 

$$
\pi^{1}_{m} = \frac{\pi^{0}_{m} BF_{mu}}{\pi^{0}_{0} BF_{0u} + \pi^{0}_{>} BF_{>u} + \pi^{0}_{<} BF_{<u}}
$$


```{r expito1}
### Posterior Model Probabilities for Hm  = pi1
# single update 
pi0.to.1<- function(pi0,BF){
  # Pi0 are the prior Model Probabilities and BF are the Bayes factors BF.mu for a single study
  # Careful not to use the whole matrix of BF's here !
  
  out<- (pi0*BF) / sum(pi0*BF) 
  out
}
```

**Summary of input / output**

__Inputs__

- pi0: Pi0 are the prior Model Probabilities.

- BF:Bayes factors for a single study across all hypotheses.

__Outputs__

The output for this function is a vector of length 3, which is the PMP for the given inputs. 

**Example Output**

```{r expito1cd,echo = FALSE}
pi0.to.1(pi0,BF.mu[,1])
```

**pi.0m.t.func(pi0,BF.mu)**

This function does the updating evidence step of the method. It implements the following: 

$$
\pi^{0}_{t,m} = \pi^{1}_{t -1,m} for t = 2, ... ,T
$$

And then uses the **pi0.to.1** to update and calculate the PMP values for the set of studies.

```{r exupdate}
## Updating evidence : 
# step 1 find pi.0m.t where t is the study number

pi.0m.t.func<- function(pi0,BF.mu){
  pmp.t<- BF.mu
  pmp.t[,1]<- pi0.to.1(pi0,BF.mu[,1]) 
  
  for(i in 2:ncol(BF.mu)){
    pmp.t[,i]<- pi0.to.1(pmp.t[,i-1],BF.mu[,i])
  }
  names.vec<- rep(0, ncol(BF.mu))
  for(i in 1:ncol(BF.mu)){
    names.vec[i]<- paste0("PMP", i )
  }
  colnames(pmp.t)<- names.vec
  pmp.t 
}

```
**Summary of input / output**

__Inputs__

- pi0: Pi0 are the prior Model Probabilities.

- BF.mu: Matrix of Bayes Factors for the unconstrained model for each hypothesis. 

__Outputs__

This outputs the matrix of PMP values and is the final output of the wrapper function:

**Example Output**

```{r expmpfin , echo = FALSE}
pi.0m.t.func(pi0,BF.mu)
```


####Wrapper Function: *PMP.func(beta1, var1,pi0,beta0, sd.mult = 1)*

The following function is a wrapper for all of the above. It implements the method as seen in Kuiper et al.

**Summary of input / output**

__Inputs__

- beta1: The predicted means of the data. 
- var1 : The predicted variances of the data.
- pi0: The prior model probablilities, default will be to give each hypothesis equal probability so $\pi_{m}^{0} = \frac{1}{3} , m \in \{0 ,>,< \}$ 
- beta0: This is the prior mean, which has been set to 0 throughout. 
- sd.mult = 1 : A multiplier of the prior variance, used in Kuiper et al to test the method. Default set at 1.

```{r PMP}

PMP.func<- function(beta1, var1,pi0,beta0, sd.mult = 1){
  var0<- prior.var(beta1,sigma = sqrt(var1))
  var0<-var0*sd.mult

  u.post.variables <- u.post.param(prior.beta = beta0,prior.var = var0,data.beta = beta1,data.var = var1) 
  beta1.post<- u.post.variables[1,]
  var1.post<- u.post.variables[2,]

  BF.mu<- BF.mu.func(prior.mean =  0, posterior.mean = beta1.post,sd.prior = sqrt(var0) , sd.posterior = sqrt(var1.post),n = 100000 )
  
  PMP<- pi.0m.t.func(pi0 = pi0,BF.mu = BF.mu)
  PMP
}

```

### Testing the code : 

I aim to reproduce the results in the Kuiper et al paper. They have used the following to produce a series of results: 

Study | $\sigma$ | $\beta$ No effect | Small Effect | Mixed Effect 1 | Mixed Effect 2 | Mixed Effect 3 
---|---|---|---|---|---|---
1 | 0.029 | 0.007 | 0.045 | 0.055 | 0.068 | 0.009
2 | 0.054 | 0.014 | 0.084 | -0.102 | -0.084 | -0.084
3 | 0.093 | 0.078 | 0.175 | 0.153 | 0.175 | 0.175
4 | 0.179 | 0.151 | 0.337 | 0.151 | 0.337 | 0.337 



------

Using these values I will run the function for the above and compare them to the results in the Kuiper et al paper.

First I will check the prior variances: 

```{r varcheck, echo = FALSE}
beta0<- 0
var1<- c(0.029,0.054,0.093,0.179)
var1<- var1^2
#No effect
beta1<- c(0.007,0.014,0.078,0.151)
NEvar0<- prior.var(beta1,sigma = sqrt(var1))

#Small effect
beta1<- c(0.045,-0.084,0.175,0.337)
SEvar0<- prior.var(beta1,sigma = sqrt(var1))

#Mixed Effect 1 
beta1<- c(0.055,-0.102,0.153,0.151)
ME1var0<- prior.var(beta1,sigma = sqrt(var1))

### mixed effect 2 
beta1<- c(0.068,-0.084,0.175,0.337)
ME2var0<- prior.var(beta1,sigma = sqrt(var1))

### mixed effect 3 
beta1<- c(0.009,-0.084,0.175,0.337)
ME3var0<- prior.var(beta1,sigma = sqrt(var1))

```

Study $\sigma^{2}_{0}$ |  No effect | Small Effect | Mixed Effect 1 | Mixed Effect 2 | Mixed Effect 3 
---|---|---|---|---|---
1 | `r NEvar0[1]` | `r SEvar0[1]` | `r ME1var0[1]` | `r ME2var0[1]` | `r ME3var0[1]` 
2 | `r NEvar0[2]` | `r SEvar0[2]` | `r ME1var0[2]` | `r ME2var0[2]` | `r ME3var0[2]` 
3 | `r NEvar0[3]` | `r SEvar0[3]` | `r ME1var0[3]` | `r ME2var0[3]` | `r ME3var0[3]`  
4 | `r NEvar0[4]` | `r SEvar0[4]` | `r ME1var0[4]` | `r ME2var0[4]` | `r ME3var0[4]`



```{r testdatasetup, echo = FALSE}
##Sigma 
# No effect page 76

beta1<- c(0.007,0.014,0.078,0.151)

NEhalf<- PMP.func(beta1,var1,pi0,beta0 = 0,0.5)[,4]
NEone<- PMP.func(beta1,var1,pi0,beta0 = 0,1)[,4]
NEtwo<- PMP.func(beta1,var1,pi0,beta0 = 0,2)[,4]

# small effect

beta1<- c(0.045,0.084,0.175,0.337)
SEhalf<- PMP.func(beta1,var1,pi0,beta0 = 0,sd.mult = 0.5)[,4]
SEone<- PMP.func(beta1,var1,pi0,beta0 = 0,sd.mult = 1)[,4]
SEtwo<- PMP.func(beta1,var1,pi0,beta0 = 0,sd.mult = 2)[,4]

# Mixed effect page 76
beta1<- c(0.055,-0.102,0.153,0.151)
ME1half<-PMP.func(beta1,var1,pi0,beta0 = 0,sd.mult = 0.5)[,4]
ME1one<-PMP.func(beta1,var1,pi0,beta0 = 0,sd.mult = 1)[,4]
ME1two<-PMP.func(beta1,var1,pi0,beta0 = 0,sd.mult = 2)[,4]

### mixed effect 2 
beta1<- c(0.068,-0.084,0.175,0.337)
ME2half<-PMP.func(beta1,var1,pi0,beta0 = 0,sd.mult = 0.5)[,4]
ME2one<-PMP.func(beta1,var1,pi0,beta0 = 0,sd.mult = 1)[,4]
ME2two<-PMP.func(beta1,var1,pi0,beta0 = 0,sd.mult = 2)[,4]

### mixed effect 3 
beta1<- c(0.009,-0.084,0.175,0.337)
ME3half<-PMP.func(beta1,var1,pi0,beta0 = 0,sd.mult = 0.5)[,4]
ME3one<-PMP.func(beta1,var1,pi0,beta0 = 0,sd.mult = 1)[,4]
ME3two<-PMP.func(beta1,var1,pi0,beta0 = 0,sd.mult = 2)[,4]



```


$H_{m}$ | $\sigma$ multiplier | No effect | Small Effect | Mixed Effect 1 | Mixed Effect 2 | Mixed Effect 3
---|---|---|---|---|---|---
$H_{0}$ | 0.5 | `r NEhalf[1]` | `r SEhalf[1]` | `r ME1half[1]` | `r ME2half[1]` | `r ME3half[1]`  
$H_{>}$ | 0.5 | `r NEhalf[2]` | `r SEhalf[2]` | `r ME1half[2]` | `r ME2half[2]` | `r ME3half[2]`  
$H_{<}$ | 0.5 | `r NEhalf[3]` | `r SEhalf[3]` | `r ME1half[3]` | `r ME2half[3]` | `r ME3half[3]` 
---|---|---|---|---|---|---
$H_{0}$ | 1 | `r NEone[1]` | `r SEone[1]` | `r ME1one[1]` | `r ME2one[1]` | `r ME3one[1]`  
$H_{>}$ | 1 | `r NEone[2]` | `r SEone[2]` | `r ME1one[2]` | `r ME2one[2]` | `r ME3one[2]`  
$H_{<}$ | 1 | `r NEone[3]` | `r SEone[3]` | `r ME1one[3]` | `r ME2one[3]` | `r ME3one[3]` 
---|---|---|---|---|---|---
$H_{0}$ | 2 | `r NEtwo[1]` | `r SEtwo[1]` | `r ME1two[1]` | `r ME2two[1]` | `r ME3two[1]`  
$H_{>}$ | 2 | `r NEtwo[2]` | `r SEtwo[2]` | `r ME1two[2]` | `r ME2two[2]` | `r ME3two[2]`  
$H_{<}$ | 2 | `r NEtwo[3]` | `r SEtwo[3]` | `r ME1two[3]` | `r ME2two[3]` | `r ME3two[3]` 

## Visualisations

This section does not compute anything else, it only visualises certain aspects of the pipeline. The plots require the package *ggplot2*. 

```{r loadgg,message=FALSE,warning=FALSE}
library(ggplot2)
```

### Plotting prior and observed confidence interavals 

```{r confint}
plot.prior.var<-function(beta,sigma,percent = 99){
  n.studies<- length(beta)
  
  data<- matrix(0, ncol = 2 , nrow = n.studies)
  y<- seq(0,1,length.out = 2*n.studies)
  y1<-y[seq_along(y)%% 2 == 0]
  y2<-y[seq_along(y)%% 2 != 0]
  percent<- (percent / 100)
  percent<- percent + (1-percent)/2
  multiplier<- qnorm(percent)
  # get min/max
  data[,1]<- beta - sigma*multiplier
  data[,2]<- beta + sigma*multiplier
  bounds<- abs(data)
  output <- apply(X = bounds,MARGIN = 1,max)
  
  X3<- 0 - output
  X4 <-0 + output
  
  data<- data.frame(data,X3,X4, y1,y2,beta)
  
  p<- ggplot(data, aes(X1,y1))
  p<-p+ geom_point(aes(X1,y1), shape = 124,size = 4) + geom_point(aes(X2,y1),shape = 124,size = 4) 
  p<- p+ geom_vline(xintercept = 0,linetype = 2 )
  p<- p + geom_segment(aes(x =X1, y = y1 , xend = X2, yend = y1))
  p<- p+ geom_point(aes(X3,y2),col = "red" ,shape = 124,size = 4) + geom_point(aes(X4,y2), col = "red",shape = 124,size = 4) 
  p<- p + geom_segment(aes(x =X3, y = y2 , xend = X4, yend = y2),col = "red")
  p<- p + geom_point(aes(beta,y1))
  p<- p+geom_text(aes(beta,y1,label = paste("beta" , "^",1,1:n.studies,"*", "'' %+-% '' ","*", round(multiplier,3),"*","sigma","^",11:14, sep = "")),parse = TRUE,nudge_y = -0.03)
  p<- p+geom_text(aes(rep(0,4),y2,label = paste("beta" , "^",1:n.studies,"*", "'' %+-% '' ","*", round(multiplier,3),"*","sigma","^",1:4, sep = "")),parse = TRUE,nudge_y = -0.03)
  p<- p + theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())
  p<- p + xlab(NULL) + ylab(NULL)
  p<- p+ ggtitle("Prior Variance Selection")
  p
}
```

An example of the output of the *plot.prior.var* function is: 

```{r exmplplot1}
beta1<- c(0.068,-0.084,0.175,0.337)
sig1<- c(0.029,0.054,0.093,0.179)
plot.prior.var(beta1,sigma = sig1)
```

### Plotting posterior model probabilities:


